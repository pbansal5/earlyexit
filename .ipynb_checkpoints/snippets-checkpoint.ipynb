{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f24218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import trl\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from peft import LoraModel, PeftModelForCausalLM, LoraConfig, PeftModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, logging, set_seed, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from peft import tuners\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import trainer\n",
    "from trl import trainer as trl_trainer\n",
    "from peft import PeftConfig, PeftModel, get_peft_model\n",
    "\n",
    "from util_code import *\n",
    "from custom_code import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae977d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e3b5f9a2047e78a59ee3cff98b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6300938918da47b2a833dd9c19992362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset civil_comments/default to /u/pbansal/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9505e8cabce045fe9ed1de2c5708c0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/415M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1804874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset civil_comments downloaded and prepared to /u/pbansal/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1780d44bc2d484c9b9f6def0b52fefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    'civil_comments',\n",
    "#     'lvwerra/stack-exchange-paired'\n",
    "#     data_dir='train',\n",
    "#     split='train',\n",
    "#     use_auth_token=True,\n",
    "#     num_proc=args.num_workers if not args.streaming else None,\n",
    "#     streaming=args.streaming,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07a15316",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets.Dataset.from_dict({'text':['dafs'],'toxicity':[0.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "754eef4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n",
       " 'toxicity': 0.0,\n",
       " 'severe_toxicity': 0.0,\n",
       " 'obscene': 0.0,\n",
       " 'threat': 0.0,\n",
       " 'insult': 0.0,\n",
       " 'identity_attack': 0.0,\n",
       " 'sexual_explicit': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b87fe08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 1804874\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.DatasetDict({'train':dataset['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ff494618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5bfba83f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "def remove_columnns(dataset):\n",
    "    texts = [x['text'] for x in dataset]\n",
    "    labels = [int(x['toxicity']>0.1) for x in dataset]\n",
    "    \n",
    "#     group1 = [i for i,(x,z) in enumerate(zip(texts,labels)) if (z == 0) and ('kill' in x)]\n",
    "#     group2 = [i for i,(x,z) in enumerate(zip(texts,labels)) if (z == 1) and ('kill' in x)]\n",
    "#     group3 = [i for i,(x,z) in enumerate(zip(texts,labels)) if (z == 0) and not ('kill' in x)]\n",
    "#     group4 = [i for i,(x,z) in enumerate(zip(texts,labels)) if (z == 1) and not ('kill' in x)]\n",
    "#     num_to_select = np.min([len(group1),len(group2),len(group3),len(group4)])\n",
    "#     selected_ = []\n",
    "#     for x in [group1,group2,group3,group4]:\n",
    "#         selected_.append(np.random.choice(x,size=(num_to_select,),replace=False))\n",
    "\n",
    "#     factor_ = float(np.sum(labels)/(len(labels)-np.sum(labels)))\n",
    "#     selected_ = [i for i,x in enumerate(labels) if ((labels[i] == 1) or np.random.uniform()<factor_)]\n",
    "\n",
    "    selected_ = [i for i,(x,z) in enumerate(zip(texts,labels)) if 'kill' in x or np.random.uniform()<0.1]\n",
    "\n",
    "    np.random.shuffle(selected_)\n",
    "    texts = [texts[i] for i in selected_]\n",
    "    labels = [labels[i] for i in selected_]\n",
    "    return datasets.Dataset.from_dict({'text': texts,'label':labels, 'group_ids'})\n",
    "\n",
    "final_dataset = dict({})\n",
    "for key in dataset.keys():\n",
    "    print (key)\n",
    "    if (key == 'train'):\n",
    "        continue\n",
    "    new_dataset = remove_columnns(dataset[key])\n",
    "    final_dataset[key] = new_dataset\n",
    "    \n",
    "final_dataset = datasets.DatasetDict(final_dataset)\n",
    "# final_dataset.save_to_disk(\"/var/local/pbansal/dumps/cc_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29da5c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3328555771014248"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(final_dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33f085b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5071459506279775"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([z for x,z in zip(final_dataset['test']['text'],final_dataset['test']['label']) if 'kill' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43c654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed155def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1946716128488323"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([z for x,z in zip(final_dataset['test']['text'],final_dataset['test']['label']) if 'kill' in x])/len(final_dataset['test'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fccfc780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1804874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset.save_to_disk(\"/var/local/pbansal/dumps/cc_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37097505",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f2 = datasets.DatasetDict.load_from_disk(\"/var/local/pbansal/dumps/cc_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a104c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29843911541747514"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f2['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3fed92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pb25659/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04/cache-8cdcbac2c8955313.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(data): \n",
    "#     print (data)\n",
    "#     print (data[\"questions\"]['text'][0])\n",
    "#     return tokenizer(' [SEP] '.join(data[\"questions\"]['text']), truncation=True)\n",
    "    return tokenizer([' [SEP] '.join(x['text']) for x in data[\"questions\"]], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50ede7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'is_duplicate'],\n",
       "        num_rows: 404290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e56f35b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.369197853026293"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(x) for x in tokenized_dataset['train']['is_duplicate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa34da75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': [1, 2],\n",
       "  'text': ['What is the step by step guide to invest in share market in india?',\n",
       "   'What is the step by step guide to invest in share market?']},\n",
       " {'id': [3, 4],\n",
       "  'text': ['What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
       "   'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?']},\n",
       " {'id': [5, 6],\n",
       "  'text': ['How can I increase the speed of my internet connection while using a VPN?',\n",
       "   'How can Internet speed be increased by hacking through DNS?']},\n",
       " {'id': [7, 8],\n",
       "  'text': ['Why am I mentally very lonely? How can I solve it?',\n",
       "   'Find the remainder when [math]23^{24}[/math] is divided by 24,23?']},\n",
       " {'id': [9, 10],\n",
       "  'text': ['Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?',\n",
       "   'Which fish would survive in salt water?']},\n",
       " {'id': [11, 12],\n",
       "  'text': ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
       "   \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"]},\n",
       " {'id': [13, 14],\n",
       "  'text': ['Should I buy tiago?',\n",
       "   'What keeps childern active and far from phone and video games?']},\n",
       " {'id': [15, 16],\n",
       "  'text': ['How can I be a good geologist?',\n",
       "   'What should I do to be a great geologist?']},\n",
       " {'id': [17, 18],\n",
       "  'text': ['When do you use シ instead of し?',\n",
       "   'When do you use \"&\" instead of \"and\"?']},\n",
       " {'id': [19, 20],\n",
       "  'text': ['Motorola (company): Can I hack my Charter Motorolla DCX3400?',\n",
       "   'How do I hack Motorola DCX3400 for free internet?']}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['questions'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e338caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Give three tips for staying healthy.',\n",
       " '',\n",
       " '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "(dataset['train']['instruction'][i],\n",
    "dataset['train']['input'][i],\n",
    "dataset['train']['output'][i],\n",
    "dataset['train']['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52844be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dee78284",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/opt-1.3b')\n",
    "model = PeftModelForCausalLM(model, lora_config, \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9838d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811ecb9ad440469faf290a2af67a69fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d84f415a4040f2a9258af776447b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-1.3b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('facebook/opt-1.3b',num_labels=2)\n",
    "model = PeftModelForSequenceClassification(model, lora_config, \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4d4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.embed_tokens.weight False\n",
      "model.decoder.embed_positions.weight False\n",
      "model.decoder.final_layer_norm.weight False\n",
      "model.decoder.final_layer_norm.bias False\n",
      "model.decoder.layers.0.self_attn.k_proj.weight False\n",
      "model.decoder.layers.0.self_attn.k_proj.bias False\n",
      "model.decoder.layers.0.self_attn.v_proj.weight False\n",
      "model.decoder.layers.0.self_attn.v_proj.bias False\n",
      "model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.0.self_attn.q_proj.weight False\n",
      "model.decoder.layers.0.self_attn.q_proj.bias False\n",
      "model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.0.self_attn.out_proj.weight False\n",
      "model.decoder.layers.0.self_attn.out_proj.bias False\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.0.fc1.weight False\n",
      "model.decoder.layers.0.fc1.bias False\n",
      "model.decoder.layers.0.fc2.weight False\n",
      "model.decoder.layers.0.fc2.bias False\n",
      "model.decoder.layers.0.final_layer_norm.weight False\n",
      "model.decoder.layers.0.final_layer_norm.bias False\n",
      "model.decoder.layers.1.self_attn.k_proj.weight False\n",
      "model.decoder.layers.1.self_attn.k_proj.bias False\n",
      "model.decoder.layers.1.self_attn.v_proj.weight False\n",
      "model.decoder.layers.1.self_attn.v_proj.bias False\n",
      "model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.1.self_attn.q_proj.weight False\n",
      "model.decoder.layers.1.self_attn.q_proj.bias False\n",
      "model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.1.self_attn.out_proj.weight False\n",
      "model.decoder.layers.1.self_attn.out_proj.bias False\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.1.fc1.weight False\n",
      "model.decoder.layers.1.fc1.bias False\n",
      "model.decoder.layers.1.fc2.weight False\n",
      "model.decoder.layers.1.fc2.bias False\n",
      "model.decoder.layers.1.final_layer_norm.weight False\n",
      "model.decoder.layers.1.final_layer_norm.bias False\n",
      "model.decoder.layers.2.self_attn.k_proj.weight False\n",
      "model.decoder.layers.2.self_attn.k_proj.bias False\n",
      "model.decoder.layers.2.self_attn.v_proj.weight False\n",
      "model.decoder.layers.2.self_attn.v_proj.bias False\n",
      "model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.2.self_attn.q_proj.weight False\n",
      "model.decoder.layers.2.self_attn.q_proj.bias False\n",
      "model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.2.self_attn.out_proj.weight False\n",
      "model.decoder.layers.2.self_attn.out_proj.bias False\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.2.fc1.weight False\n",
      "model.decoder.layers.2.fc1.bias False\n",
      "model.decoder.layers.2.fc2.weight False\n",
      "model.decoder.layers.2.fc2.bias False\n",
      "model.decoder.layers.2.final_layer_norm.weight False\n",
      "model.decoder.layers.2.final_layer_norm.bias False\n",
      "model.decoder.layers.3.self_attn.k_proj.weight False\n",
      "model.decoder.layers.3.self_attn.k_proj.bias False\n",
      "model.decoder.layers.3.self_attn.v_proj.weight False\n",
      "model.decoder.layers.3.self_attn.v_proj.bias False\n",
      "model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.3.self_attn.q_proj.weight False\n",
      "model.decoder.layers.3.self_attn.q_proj.bias False\n",
      "model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.3.self_attn.out_proj.weight False\n",
      "model.decoder.layers.3.self_attn.out_proj.bias False\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.3.fc1.weight False\n",
      "model.decoder.layers.3.fc1.bias False\n",
      "model.decoder.layers.3.fc2.weight False\n",
      "model.decoder.layers.3.fc2.bias False\n",
      "model.decoder.layers.3.final_layer_norm.weight False\n",
      "model.decoder.layers.3.final_layer_norm.bias False\n",
      "model.decoder.layers.4.self_attn.k_proj.weight False\n",
      "model.decoder.layers.4.self_attn.k_proj.bias False\n",
      "model.decoder.layers.4.self_attn.v_proj.weight False\n",
      "model.decoder.layers.4.self_attn.v_proj.bias False\n",
      "model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.4.self_attn.q_proj.weight False\n",
      "model.decoder.layers.4.self_attn.q_proj.bias False\n",
      "model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.4.self_attn.out_proj.weight False\n",
      "model.decoder.layers.4.self_attn.out_proj.bias False\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.4.fc1.weight False\n",
      "model.decoder.layers.4.fc1.bias False\n",
      "model.decoder.layers.4.fc2.weight False\n",
      "model.decoder.layers.4.fc2.bias False\n",
      "model.decoder.layers.4.final_layer_norm.weight False\n",
      "model.decoder.layers.4.final_layer_norm.bias False\n",
      "model.decoder.layers.5.self_attn.k_proj.weight False\n",
      "model.decoder.layers.5.self_attn.k_proj.bias False\n",
      "model.decoder.layers.5.self_attn.v_proj.weight False\n",
      "model.decoder.layers.5.self_attn.v_proj.bias False\n",
      "model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.5.self_attn.q_proj.weight False\n",
      "model.decoder.layers.5.self_attn.q_proj.bias False\n",
      "model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.5.self_attn.out_proj.weight False\n",
      "model.decoder.layers.5.self_attn.out_proj.bias False\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.5.fc1.weight False\n",
      "model.decoder.layers.5.fc1.bias False\n",
      "model.decoder.layers.5.fc2.weight False\n",
      "model.decoder.layers.5.fc2.bias False\n",
      "model.decoder.layers.5.final_layer_norm.weight False\n",
      "model.decoder.layers.5.final_layer_norm.bias False\n",
      "model.decoder.layers.6.self_attn.k_proj.weight False\n",
      "model.decoder.layers.6.self_attn.k_proj.bias False\n",
      "model.decoder.layers.6.self_attn.v_proj.weight False\n",
      "model.decoder.layers.6.self_attn.v_proj.bias False\n",
      "model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.6.self_attn.q_proj.weight False\n",
      "model.decoder.layers.6.self_attn.q_proj.bias False\n",
      "model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.6.self_attn.out_proj.weight False\n",
      "model.decoder.layers.6.self_attn.out_proj.bias False\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.6.fc1.weight False\n",
      "model.decoder.layers.6.fc1.bias False\n",
      "model.decoder.layers.6.fc2.weight False\n",
      "model.decoder.layers.6.fc2.bias False\n",
      "model.decoder.layers.6.final_layer_norm.weight False\n",
      "model.decoder.layers.6.final_layer_norm.bias False\n",
      "model.decoder.layers.7.self_attn.k_proj.weight False\n",
      "model.decoder.layers.7.self_attn.k_proj.bias False\n",
      "model.decoder.layers.7.self_attn.v_proj.weight False\n",
      "model.decoder.layers.7.self_attn.v_proj.bias False\n",
      "model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.7.self_attn.q_proj.weight False\n",
      "model.decoder.layers.7.self_attn.q_proj.bias False\n",
      "model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.7.self_attn.out_proj.weight False\n",
      "model.decoder.layers.7.self_attn.out_proj.bias False\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.7.fc1.weight False\n",
      "model.decoder.layers.7.fc1.bias False\n",
      "model.decoder.layers.7.fc2.weight False\n",
      "model.decoder.layers.7.fc2.bias False\n",
      "model.decoder.layers.7.final_layer_norm.weight False\n",
      "model.decoder.layers.7.final_layer_norm.bias False\n",
      "model.decoder.layers.8.self_attn.k_proj.weight False\n",
      "model.decoder.layers.8.self_attn.k_proj.bias False\n",
      "model.decoder.layers.8.self_attn.v_proj.weight False\n",
      "model.decoder.layers.8.self_attn.v_proj.bias False\n",
      "model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.8.self_attn.q_proj.weight False\n",
      "model.decoder.layers.8.self_attn.q_proj.bias False\n",
      "model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.8.self_attn.out_proj.weight False\n",
      "model.decoder.layers.8.self_attn.out_proj.bias False\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.8.fc1.weight False\n",
      "model.decoder.layers.8.fc1.bias False\n",
      "model.decoder.layers.8.fc2.weight False\n",
      "model.decoder.layers.8.fc2.bias False\n",
      "model.decoder.layers.8.final_layer_norm.weight False\n",
      "model.decoder.layers.8.final_layer_norm.bias False\n",
      "model.decoder.layers.9.self_attn.k_proj.weight False\n",
      "model.decoder.layers.9.self_attn.k_proj.bias False\n",
      "model.decoder.layers.9.self_attn.v_proj.weight False\n",
      "model.decoder.layers.9.self_attn.v_proj.bias False\n",
      "model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.9.self_attn.q_proj.weight False\n",
      "model.decoder.layers.9.self_attn.q_proj.bias False\n",
      "model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.9.self_attn.out_proj.weight False\n",
      "model.decoder.layers.9.self_attn.out_proj.bias False\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.9.fc1.weight False\n",
      "model.decoder.layers.9.fc1.bias False\n",
      "model.decoder.layers.9.fc2.weight False\n",
      "model.decoder.layers.9.fc2.bias False\n",
      "model.decoder.layers.9.final_layer_norm.weight False\n",
      "model.decoder.layers.9.final_layer_norm.bias False\n",
      "model.decoder.layers.10.self_attn.k_proj.weight False\n",
      "model.decoder.layers.10.self_attn.k_proj.bias False\n",
      "model.decoder.layers.10.self_attn.v_proj.weight False\n",
      "model.decoder.layers.10.self_attn.v_proj.bias False\n",
      "model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.10.self_attn.q_proj.weight False\n",
      "model.decoder.layers.10.self_attn.q_proj.bias False\n",
      "model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.10.self_attn.out_proj.weight False\n",
      "model.decoder.layers.10.self_attn.out_proj.bias False\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.10.fc1.weight False\n",
      "model.decoder.layers.10.fc1.bias False\n",
      "model.decoder.layers.10.fc2.weight False\n",
      "model.decoder.layers.10.fc2.bias False\n",
      "model.decoder.layers.10.final_layer_norm.weight False\n",
      "model.decoder.layers.10.final_layer_norm.bias False\n",
      "model.decoder.layers.11.self_attn.k_proj.weight False\n",
      "model.decoder.layers.11.self_attn.k_proj.bias False\n",
      "model.decoder.layers.11.self_attn.v_proj.weight False\n",
      "model.decoder.layers.11.self_attn.v_proj.bias False\n",
      "model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.11.self_attn.q_proj.weight False\n",
      "model.decoder.layers.11.self_attn.q_proj.bias False\n",
      "model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.11.self_attn.out_proj.weight False\n",
      "model.decoder.layers.11.self_attn.out_proj.bias False\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.11.fc1.weight False\n",
      "model.decoder.layers.11.fc1.bias False\n",
      "model.decoder.layers.11.fc2.weight False\n",
      "model.decoder.layers.11.fc2.bias False\n",
      "model.decoder.layers.11.final_layer_norm.weight False\n",
      "model.decoder.layers.11.final_layer_norm.bias False\n",
      "model.decoder.layers.12.self_attn.k_proj.weight False\n",
      "model.decoder.layers.12.self_attn.k_proj.bias False\n",
      "model.decoder.layers.12.self_attn.v_proj.weight False\n",
      "model.decoder.layers.12.self_attn.v_proj.bias False\n",
      "model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.12.self_attn.q_proj.weight False\n",
      "model.decoder.layers.12.self_attn.q_proj.bias False\n",
      "model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.12.self_attn.out_proj.weight False\n",
      "model.decoder.layers.12.self_attn.out_proj.bias False\n",
      "model.decoder.layers.12.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.12.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.12.fc1.weight False\n",
      "model.decoder.layers.12.fc1.bias False\n",
      "model.decoder.layers.12.fc2.weight False\n",
      "model.decoder.layers.12.fc2.bias False\n",
      "model.decoder.layers.12.final_layer_norm.weight False\n",
      "model.decoder.layers.12.final_layer_norm.bias False\n",
      "model.decoder.layers.13.self_attn.k_proj.weight False\n",
      "model.decoder.layers.13.self_attn.k_proj.bias False\n",
      "model.decoder.layers.13.self_attn.v_proj.weight False\n",
      "model.decoder.layers.13.self_attn.v_proj.bias False\n",
      "model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.13.self_attn.q_proj.weight False\n",
      "model.decoder.layers.13.self_attn.q_proj.bias False\n",
      "model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.13.self_attn.out_proj.weight False\n",
      "model.decoder.layers.13.self_attn.out_proj.bias False\n",
      "model.decoder.layers.13.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.13.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.13.fc1.weight False\n",
      "model.decoder.layers.13.fc1.bias False\n",
      "model.decoder.layers.13.fc2.weight False\n",
      "model.decoder.layers.13.fc2.bias False\n",
      "model.decoder.layers.13.final_layer_norm.weight False\n",
      "model.decoder.layers.13.final_layer_norm.bias False\n",
      "model.decoder.layers.14.self_attn.k_proj.weight False\n",
      "model.decoder.layers.14.self_attn.k_proj.bias False\n",
      "model.decoder.layers.14.self_attn.v_proj.weight False\n",
      "model.decoder.layers.14.self_attn.v_proj.bias False\n",
      "model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.14.self_attn.q_proj.weight False\n",
      "model.decoder.layers.14.self_attn.q_proj.bias False\n",
      "model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.14.self_attn.out_proj.weight False\n",
      "model.decoder.layers.14.self_attn.out_proj.bias False\n",
      "model.decoder.layers.14.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.14.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.14.fc1.weight False\n",
      "model.decoder.layers.14.fc1.bias False\n",
      "model.decoder.layers.14.fc2.weight False\n",
      "model.decoder.layers.14.fc2.bias False\n",
      "model.decoder.layers.14.final_layer_norm.weight False\n",
      "model.decoder.layers.14.final_layer_norm.bias False\n",
      "model.decoder.layers.15.self_attn.k_proj.weight False\n",
      "model.decoder.layers.15.self_attn.k_proj.bias False\n",
      "model.decoder.layers.15.self_attn.v_proj.weight False\n",
      "model.decoder.layers.15.self_attn.v_proj.bias False\n",
      "model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.15.self_attn.q_proj.weight False\n",
      "model.decoder.layers.15.self_attn.q_proj.bias False\n",
      "model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.15.self_attn.out_proj.weight False\n",
      "model.decoder.layers.15.self_attn.out_proj.bias False\n",
      "model.decoder.layers.15.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.15.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.15.fc1.weight False\n",
      "model.decoder.layers.15.fc1.bias False\n",
      "model.decoder.layers.15.fc2.weight False\n",
      "model.decoder.layers.15.fc2.bias False\n",
      "model.decoder.layers.15.final_layer_norm.weight False\n",
      "model.decoder.layers.15.final_layer_norm.bias False\n",
      "model.decoder.layers.16.self_attn.k_proj.weight False\n",
      "model.decoder.layers.16.self_attn.k_proj.bias False\n",
      "model.decoder.layers.16.self_attn.v_proj.weight False\n",
      "model.decoder.layers.16.self_attn.v_proj.bias False\n",
      "model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.16.self_attn.q_proj.weight False\n",
      "model.decoder.layers.16.self_attn.q_proj.bias False\n",
      "model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.16.self_attn.out_proj.weight False\n",
      "model.decoder.layers.16.self_attn.out_proj.bias False\n",
      "model.decoder.layers.16.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.16.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.16.fc1.weight False\n",
      "model.decoder.layers.16.fc1.bias False\n",
      "model.decoder.layers.16.fc2.weight False\n",
      "model.decoder.layers.16.fc2.bias False\n",
      "model.decoder.layers.16.final_layer_norm.weight False\n",
      "model.decoder.layers.16.final_layer_norm.bias False\n",
      "model.decoder.layers.17.self_attn.k_proj.weight False\n",
      "model.decoder.layers.17.self_attn.k_proj.bias False\n",
      "model.decoder.layers.17.self_attn.v_proj.weight False\n",
      "model.decoder.layers.17.self_attn.v_proj.bias False\n",
      "model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.17.self_attn.q_proj.weight False\n",
      "model.decoder.layers.17.self_attn.q_proj.bias False\n",
      "model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.17.self_attn.out_proj.weight False\n",
      "model.decoder.layers.17.self_attn.out_proj.bias False\n",
      "model.decoder.layers.17.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.17.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.17.fc1.weight False\n",
      "model.decoder.layers.17.fc1.bias False\n",
      "model.decoder.layers.17.fc2.weight False\n",
      "model.decoder.layers.17.fc2.bias False\n",
      "model.decoder.layers.17.final_layer_norm.weight False\n",
      "model.decoder.layers.17.final_layer_norm.bias False\n",
      "model.decoder.layers.18.self_attn.k_proj.weight False\n",
      "model.decoder.layers.18.self_attn.k_proj.bias False\n",
      "model.decoder.layers.18.self_attn.v_proj.weight False\n",
      "model.decoder.layers.18.self_attn.v_proj.bias False\n",
      "model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.18.self_attn.q_proj.weight False\n",
      "model.decoder.layers.18.self_attn.q_proj.bias False\n",
      "model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.18.self_attn.out_proj.weight False\n",
      "model.decoder.layers.18.self_attn.out_proj.bias False\n",
      "model.decoder.layers.18.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.18.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.18.fc1.weight False\n",
      "model.decoder.layers.18.fc1.bias False\n",
      "model.decoder.layers.18.fc2.weight False\n",
      "model.decoder.layers.18.fc2.bias False\n",
      "model.decoder.layers.18.final_layer_norm.weight False\n",
      "model.decoder.layers.18.final_layer_norm.bias False\n",
      "model.decoder.layers.19.self_attn.k_proj.weight False\n",
      "model.decoder.layers.19.self_attn.k_proj.bias False\n",
      "model.decoder.layers.19.self_attn.v_proj.weight False\n",
      "model.decoder.layers.19.self_attn.v_proj.bias False\n",
      "model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.19.self_attn.q_proj.weight False\n",
      "model.decoder.layers.19.self_attn.q_proj.bias False\n",
      "model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.19.self_attn.out_proj.weight False\n",
      "model.decoder.layers.19.self_attn.out_proj.bias False\n",
      "model.decoder.layers.19.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.19.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.19.fc1.weight False\n",
      "model.decoder.layers.19.fc1.bias False\n",
      "model.decoder.layers.19.fc2.weight False\n",
      "model.decoder.layers.19.fc2.bias False\n",
      "model.decoder.layers.19.final_layer_norm.weight False\n",
      "model.decoder.layers.19.final_layer_norm.bias False\n",
      "model.decoder.layers.20.self_attn.k_proj.weight False\n",
      "model.decoder.layers.20.self_attn.k_proj.bias False\n",
      "model.decoder.layers.20.self_attn.v_proj.weight False\n",
      "model.decoder.layers.20.self_attn.v_proj.bias False\n",
      "model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.20.self_attn.q_proj.weight False\n",
      "model.decoder.layers.20.self_attn.q_proj.bias False\n",
      "model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.20.self_attn.out_proj.weight False\n",
      "model.decoder.layers.20.self_attn.out_proj.bias False\n",
      "model.decoder.layers.20.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.20.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.20.fc1.weight False\n",
      "model.decoder.layers.20.fc1.bias False\n",
      "model.decoder.layers.20.fc2.weight False\n",
      "model.decoder.layers.20.fc2.bias False\n",
      "model.decoder.layers.20.final_layer_norm.weight False\n",
      "model.decoder.layers.20.final_layer_norm.bias False\n",
      "model.decoder.layers.21.self_attn.k_proj.weight False\n",
      "model.decoder.layers.21.self_attn.k_proj.bias False\n",
      "model.decoder.layers.21.self_attn.v_proj.weight False\n",
      "model.decoder.layers.21.self_attn.v_proj.bias False\n",
      "model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.21.self_attn.q_proj.weight False\n",
      "model.decoder.layers.21.self_attn.q_proj.bias False\n",
      "model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.21.self_attn.out_proj.weight False\n",
      "model.decoder.layers.21.self_attn.out_proj.bias False\n",
      "model.decoder.layers.21.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.21.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.21.fc1.weight False\n",
      "model.decoder.layers.21.fc1.bias False\n",
      "model.decoder.layers.21.fc2.weight False\n",
      "model.decoder.layers.21.fc2.bias False\n",
      "model.decoder.layers.21.final_layer_norm.weight False\n",
      "model.decoder.layers.21.final_layer_norm.bias False\n",
      "model.decoder.layers.22.self_attn.k_proj.weight False\n",
      "model.decoder.layers.22.self_attn.k_proj.bias False\n",
      "model.decoder.layers.22.self_attn.v_proj.weight False\n",
      "model.decoder.layers.22.self_attn.v_proj.bias False\n",
      "model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.22.self_attn.q_proj.weight False\n",
      "model.decoder.layers.22.self_attn.q_proj.bias False\n",
      "model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.22.self_attn.out_proj.weight False\n",
      "model.decoder.layers.22.self_attn.out_proj.bias False\n",
      "model.decoder.layers.22.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.22.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.22.fc1.weight False\n",
      "model.decoder.layers.22.fc1.bias False\n",
      "model.decoder.layers.22.fc2.weight False\n",
      "model.decoder.layers.22.fc2.bias False\n",
      "model.decoder.layers.22.final_layer_norm.weight False\n",
      "model.decoder.layers.22.final_layer_norm.bias False\n",
      "model.decoder.layers.23.self_attn.k_proj.weight False\n",
      "model.decoder.layers.23.self_attn.k_proj.bias False\n",
      "model.decoder.layers.23.self_attn.v_proj.weight False\n",
      "model.decoder.layers.23.self_attn.v_proj.bias False\n",
      "model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight True\n",
      "model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight True\n",
      "model.decoder.layers.23.self_attn.q_proj.weight False\n",
      "model.decoder.layers.23.self_attn.q_proj.bias False\n",
      "model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight True\n",
      "model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight True\n",
      "model.decoder.layers.23.self_attn.out_proj.weight False\n",
      "model.decoder.layers.23.self_attn.out_proj.bias False\n",
      "model.decoder.layers.23.self_attn_layer_norm.weight False\n",
      "model.decoder.layers.23.self_attn_layer_norm.bias False\n",
      "model.decoder.layers.23.fc1.weight False\n",
      "model.decoder.layers.23.fc1.bias False\n",
      "model.decoder.layers.23.fc2.weight False\n",
      "model.decoder.layers.23.fc2.bias False\n",
      "model.decoder.layers.23.final_layer_norm.weight False\n",
      "model.decoder.layers.23.final_layer_norm.bias False\n",
      "score.original_module.weight True\n",
      "score.modules_to_save.default.weight True\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.base_model.model.named_parameters():\n",
    "    print (n,p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c615e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight False\n"
     ]
    }
   ],
   "source": [
    "for n,m in model.lm_head.named_parameters():\n",
    "    print (n,m.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ee6b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForSequenceClassification(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (v_proj): Linear(\n",
       "                  in_features=2048, out_features=2048, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=1, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=1, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): Linear(\n",
       "                  in_features=2048, out_features=2048, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=1, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=1, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=2048, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c19c098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102957056"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*50272"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
